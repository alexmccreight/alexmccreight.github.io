[
  {
    "path": "posts/BayesCapstone/",
    "title": "Domestic Box Office Revenue - A Bayesian Regression and Network Approach",
    "description": "Domestic Box Office Revenue Analysis with Bayesian Regression and Bayesian Networks",
    "author": [],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\nIntroduction\nMotivation and Research Question\nMovies are one of the most important cultural products with increasing demand (Lee and Chang, 2009). Beyond entertainment, movies also take the form of serious art with profound social and philosophical reflections. Demand for specific movies is often hard to predict despite overall increasing demand. To understand how the movie industry operates and the specific factors that impact consumer preference for movies, we decided to analyze the US box office dataset and study what factors are most related to box office performance from the industry’s perspective.\nData\nData Source\nOur group is using the boxofficemojo dataset from Kaggle. The dataset contains movie data from the US box office from January 1990 to April 2020. The author, Igor Kirko, scraped movies from the boxofficemojo website that had a budget listed, a US MPAA rating, and is available in English.\nData Scraping & Cleaning\nIn our initial assessment of the data, we knew that we would need more variables for our analysis. We specifically were looking for data about the movie’s rating by the community to see if it affected its box office revenue. In 2008, Amazon purchased boxofficemojo through its subsidiary IMDB, so the two databases share the same movie codes. This allows us to utilize rating data from the IMDB website and join it with the boxofficemojo dataset.\nTo clean our dataset, we dropped all variables that would have little to no effect on box office revenue (e.g., the movie’s cinematographer, composer, and run time). We also created indicator variables such as “HolidaySeason” to determine whether or not a movie was released during what we defined as the holiday season, and “director_power,” which finds out whether a movie’s director is in the top 100 grossing directors for the worldwide box office. In total, we have 2821 movies in our dataset.\nMore details about our variables can be found in the code book section of the appendix.\nData Visualizations\nDependent Variable\nOur analysis uses domestic (total domestic box office revenue in millions) from our dataset as our dependent variable, which indicates a movie’s success.\n\n\n\nAfter evaluating the distribution of domestic box office revenue, we found that most movies generate around 70 million USD. However, the domestic box office revenue range varies from 1.228 million USD to 936.662 million USD, and the distribution is heavily right-skewed, so we decided to use the log-scale to normalize our curve.\nWe can see from the graph that the logged domestic box office revenue has a relatively normal distribution, centered around 3.75, which is roughly 42.5 million USD (\\(e^{3.75}\\)).\nQuantitative Variables\n\n\n\nOur dataset has three quantitative variables: budget, number of votes, and average rating. We wanted to view their relationships with domestic box office revenue to see whether or not they affect it. To improve our visualization, we decided to use the logged budget and the logged number of votes of a movie to decrease the variability in these variables.\nFor our first two plots, we have a positive trend between logged budget and logged domestic box office revenue and between the logged number of votes of a movie and the logged domestic box office revenue. This means that as the logged budget or the logged number of votes increases, the logged domestic box office revenue will also increase. Our third plot does not show a clear relationship between the average rating of a movie and the logged domestic box office revenue.\nCategorical Variables\n\n\n\nOur dataset has four indicator variables: director power, producer power, distributor power, and holiday season. We wanted to view their relationship with domestic box office revenue to see whether or not they affect it. To improve our visualization, we chose to use the logged domestic box office revenue to eliminate many of the outliers.\nFrom the graphs, we can see that the four variables do not significantly impact domestic box office revenue. The variability of all the graphs is roughly the same.\n\n\n\nWe have two categorical variables in our dataset with greater than two categories: actor power (0, 1, 2, 3, 4) and MPAA rating (NA, G, PG, PG-13, R, NC-17). We wanted to view their relationship with domestic box office revenue to see whether or not they affect it. To improve our visualization, we chose to use the logged domestic box office revenue to eliminate many of the outliers.\nThe graphs show that casting more actors or movies with a G MPAA rating tends to have a higher domestic box office revenue.\n\nBayesian Normal Regression Approach\nWhy This Approach?\nOur dependent variable is quantitative and bell-shaped\nIncorporate our prior knowledge into our model\nGoals\nWe are going to create two Bayesian Normal regression models to try and predict the logged domestic box office revenue. For our first model, we hope to create a model that accurately predicts the logged domestic box office revenue solely with the logged budget. For the second model, we added the variables from the data visualization section above that we believe will affect the logged domestic box office revenue.\nModel 1\nFor our first model, we will examine the relationship between the logged budget of a movie and its logged domestic box office revenue. Intuitively, we think the budget will play an essential role in the success of a movie.\nBuilding the Model\nTo model response variable Y, the logged domestic box revenue, by predictor X, the logged budget, we collected data from 2821 movies. Let \\((Y_i, X_i)\\) denote the observed data on each movie \\(i \\in \\{1, 2, ..., 2821\\}\\).\n\\[\\begin{gather}\nY_i| \\beta_0,  \\beta_1,\\sigma\\sim N(\\mu_i, \\sigma^2) \\text{   where   } \\mu_i = \\beta_0+\\beta_1X_i\\\\\n\\beta_{0c} \\sim N(m_0, s_0^2)\\\\\n\\beta_1 \\sim N(m_1, s_1^2)\\\\\n\\sigma \\sim Exp(l)\n\\end{gather}\\]\n\\(\\sigma\\) measure the typical deviation of an observed Y from the regression line \\(\\beta_0+\\beta_1X_1\\).\nPrior Analysis\n\n\nmodel_1_prior <- stan_glm(\n  log(domestic) ~ log(budget),\n  data = imdb_movies,\n  family = gaussian,\n  prior_PD = TRUE,\n  chains = 4, iter = 5000*2, seed = 84735,\n  refresh = 0)\n\n\n\n\n\n\nThe slopes of the prior plausible regression lines are both positive and negative, and the intercepts are quite varied. The priors that we have are uninformative.\nPosterior Analysis\n\n\nmodel_1 <- stan_glm(\n  log(domestic) ~ log(budget),\n  data = imdb_movies,\n  family = gaussian,\n  chains = 4, iter = 5000*2, seed = 84735,\n  refresh = 0)\n\n\n\n\n\n\nThe model accurately captures the shape and the range of the data. Also, there is much more certainty when looking at our plausible regression lines, as all of the slopes and intercepts are roughly the same.\nModel 2\nFor our second model, we are going to examine the relationship between the logged budget of a movie, the distributor’s power, the director’s power, the producer’s power, the logged average rating of a movie, the logged number of votes for a movie, the sum of the actor’s power, whether or not the movie was released during the holiday season, the movie’s MPAA rating, and the logged domestic box office revenue.\nBuilding the Model\nTo model response variable \\(Y\\), the logged domestic box revenue, by predictor \\(X_1\\), the logged budget, \\(X_2\\), distributor_powerTRUE, \\(X_3\\), director_powerTRUE, \\(X_4\\), the logged average rating, \\(X_5\\), the logged number of votes, \\(X_6\\), A_power1, \\(X_7\\), A_power2, \\(X_8\\), A_power3, \\(X_9\\), A_power4, \\(X_{10}\\), HolidaySeasonTRUE, \\(X_{11}\\), mpaa_factorNA, \\(X_{12}\\), mpaa_factorNC-17, \\(X_{13}\\), mpaa_factorPG, \\(X_{14}\\), mpaa_factorPG-13, \\(X_{15}\\), mpaa_factorR, \\(X_{16}\\), producer_powerTRUE, we collected data from 2821 movies. Let \\((Y_i, X_{ip})\\) denote the observed data on each movie \\(i \\in \\{1, 2, ..., 2821\\}\\) for each predictor \\(p \\in \\{1, 2, ..., 16\\}\\).\n\\[\\begin{gather}\nY_i| \\beta_0,  \\beta_1,\\beta_2, ...,\\beta_{16},\\sigma\\sim N(\\mu_i, \\sigma^2) \\text{   where   } \\mu_i = \\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2}+...+\\beta_{16}X_{i16}\\\\\n\\beta_{0c} \\sim N(m_0, s_0^2)\\\\\n\\beta_1 \\sim N(m_1, s_1^2)\\\\\n\\beta_2 \\sim N(m_2, s_2^2)\\\\\n\\vdots\\\\\n\\beta_{16} \\sim N(m_{16}, s_{16}^2)\\\\\n\\sigma \\sim Exp(l)\n\\end{gather}\\]\n\\(\\sigma\\) measure the typical deviation of an observed Y from the regression line \\(\\beta_0+\\beta_1X_1\\).\nWe interpret \\(\\beta_0\\), -2.194 (which equates to roughly 54,258 USD), as the estimated logged domestic box office revenue when a movie has a logged budget of 0, does not have a top 10 distributor, does not have a top 100 grossing director, does not have a top 100 grossing producer, has a logged average rating of 0, has no logged votes, has no actors with a previous box office movie, was released outside of the holiday season, and is rated G. Prior Analysis\n\n\nmodel_2_prior <- stan_glm(\n  log(domestic) ~ log(budget) + distributor_power + director_power + log(averageRating) + log(numVotes) + A_power + HolidaySeason + mpaa_factor + producer_power, \n  data = imdb_movies,\n  family = gaussian,\n  prior_PD = TRUE,\n  chains = 4, iter = 5000*2, seed = 84735,\n  refresh = 0)\n\n\n\n\n\n\nFrom the graph, we can see the the prior predictive check has a much wider range than the data, and it fails to capture the shape of the data. Thus the priors are vague.\nPosterior Analysis\n\n\nmodel_2 <- stan_glm(\n  log(domestic) ~ log(budget) + distributor_power + director_power + producer_power + log(averageRating) + log(numVotes) + A_power + HolidaySeason + mpaa_factor, \n  data = imdb_movies,\n  family = gaussian,\n  chains = 4, iter = 5000*2, seed = 84735,\n  refresh = 0)\n\n\n\n\n\n\nThe posterior predictive check accurately captures the normality and the observed range of the logged domestic box office revenue while the density chain overlay plots produce similar output.\nModel Comparison\nNow, we are going to compare these two models by using expected log-predictive density (ELPD), which measures the average log posterior predictive pdf, \\(log(f(y_{new}|\\vec{y}))\\), across all possible new data points \\(y_{new}\\) (Johnson, Ott, Dogucu).\n\n\nloo_model_1 <- loo(model_1)\nloo_model_2 <- loo(model_2)\n\n\n\n\n\nc(loo_model_1$estimates[1], loo_model_2$estimates[1])\n\n\n[1] -3959.588 -3174.781\n\nloo_compare(loo_model_1, loo_model_2)\n\n\n        elpd_diff se_diff\nmodel_2    0.0       0.0 \nmodel_1 -784.8      41.7 \n\nThe posterior predictive accuracy of model 2 is significantly greater than that of model 1 as the ELPD of model 2, -3174.781, is greater than the ELPD of model 1, -3959.588, and because the difference in the ELPD model 2 and the ELPD of model is at least two standard errors below 0.\n\nBayesian Network Approach\nIntroduction\nIn our Bayesian normal regression, our task was to predict the box office of any given movie. However, regression tasks cannot be used to draw any causal inference, nor are we informed on the interaction or inter-relations between the variables. In our data context, the causal relationships among different variables are intricate and complicated - director power, actor power, budget, and other factors could impact each other, and we do not know if the box office is directly or indirectly determined by just a subset of the variables or all of them. We moved beyond regression to graphical probabilistic tools such as Bayesian belief networks to gain more such information.\nWhat are Bayesian Networks?\nBayesian networks are graphical models (Korb and Nicholson 2004)\nnodes represent random variables\narrows represent probabilistic dependencies between them\nThe graphic structure $  = (, A) $ of a Bayesian network is a directed acyclic graph (DAG), where \\(V\\) is the node set and A is the arc set.\nDAG defines a factorization of the joint probability distribution, this is the notation for discrete random variables.\n\\[P(X_1, ..., X_v) = \\prod^{v}_{i=1}P(X_i |Π_{X_i}) \\]\nA detailed explanation of Bayesian Network\nBayesian Belief Network(BBN) is a graphical probabilistic model that specializes in deriving inferences on the causal relationships between different variables. BBN is a natural advancement of the Naive Bayes classifier, which assumes that all variables are independent. In reality, this is often not a fair assumption to make, as the variables are oftentimes correlated. However, it is also likely intractable to obtain exact causal inference of every possible pair of variables. BBN’s advantage is that it builds upon a computationally more efficient directed acyclic graph for modeling networks of causality, thus achieving higher predictive accuracy and more sensible interpretation of relationships between variables.\nA BBN graph consists of two central elements: nodes and arcs. Each node represents a random variable, and each directed arc represents causality. Absence of arc represents (conditional) independence. An example of a BBN graph is given below.\nOf all variables, bad weather and a bad headache do not have any parent nodes, and are thus independent from each other. Other variables are all somewhat related and dependent, and it would be wrong to use a Naive Bayes classifier to determine if one would fail the exam by all other variables. One key concept to BBN is conditional independence: each note is conditionally independent of its non-immediate descendents. For example, typically, if you have slept over the alarm clock, you are likely to miss and thus fail the exam. However, given certain information that you have arrived at school on time, sleeping over the alarm doesn’t necessarily have anything to do with whether you would fail the exam. This in general provides a much more sensible interpretation of variable relationships than arbitrarily determining that all variables potentially equally contribute to the outcome.\nUsually, on a given matter, we can rely on expertise or prior knowledge to design the BBN model architecture, which could be both advantageous and disadvantageous depending on dimensionality. While BBN is generally good in modeling causality, the model cannot depict cyclic or mutual relationships, which might still be problematic in some settings. For example, if failing the exam might bring you a headache and we draw a causal arc from failing the exam back to a bad headache, then BBN would crash as there’s a Headache->SleepOverAlarm->FailTheExam->Headache cyclic graph. In our data context of determining movie success, this actually could be one of the concerns: oftentimes, the producer/director/actor success and movie success are much correlated with mutual causality: good directors could make great movies with good box office; or people see a movie and thus make the actors/directors rank higher.\n\nWhy we chose this approach?\nEasily reflect domain knowledge and can simply notice cause and effect (Antal, Fannes, Timmerman, Moreau, & De Moor, 2004; Aronsky & Haug, 2000)\nLook at interdependencies (patterns of connections and effects), which means they do not face problems with over-parameterization in regression (adversely affected by adding extra variables).\nGood at dealing with missing data. Because networks are based on how variables align with each other, they will use any information that is available\nWhat is the purpose of learning the structure?\nhelps in learning the dependency structure relative to variables in a domain\nestimate the statistical model of the underlying distribution\nBayesian network structure learning algorithms\nFitting BN models\nMaro Scutari(2020), the author of bnlearn R package, outlined the process of fitting a bayesian network into two main steps: 1. Structural learning, which finds the directed acyclic graph that reflects the causality and conditional independence relations in the data; 2. Parameter learning, which estimates parameters of the global distribution. This could be derived from local distributions.\nStructural learning algorithm\nA Bayesian network, \\(\\mathbb{G}=(\\mathbb{V},A)\\), where \\(\\mathbb{V}\\) denotes nodes and \\(A\\) denotes arcs, can be shaped by expert knowledge or learned by data. There are generally two types of structural learning algorithms: score-based and constraint-based, though many hybrid approaches also use constraint-based learning to determine the approximate graph structure and use score-based algorithms to determine the best model. In this study, we will employ the hill-climbing algorithm, a greedy score-based algorithm.\n2.1 Score-based algorithms\nThe underlying mechanism behind score-based algorithms is that they will give a score to each graphical structure to measure the compatibility of the data and the structure(Kuleshov & Ermon). There are many different score measures, but a general formula of score can be written as: \\[Score(\\mathbb{G},\\mathbb{D})=L(\\mathbb{G},\\mathbb{D})-\\psi(|\\mathbb{D}|)|\\mathbb{G}|\\] where \\(L(\\mathbb{G},\\mathbb{D})\\) represents the likelihood(often logged) of data, \\(|\\mathbb{D}|\\) represents the data sample size, and \\(|\\mathbb{G}|\\) denotes the number of parameters in the given structure. Overall, \\(\\psi(|\\mathbb{D}|)|\\mathbb{G}|\\) is a penalty term that penalizes overly complex structures, thus preventing overfitting of the data. One common choice of score is to take the penalty term as \\(\\frac{|\\mathbb{D}|}{2}\\ln|\\mathbb{G}|\\), which is known as the Bayesian Information Criteria(BIC). A score-based method attempts to find the network structure that maximizes the score or finds the structure most compatible with the data.\nHowever, to run the score-based approach through all possible structures is computationally infeasible in all but the most trivial data samples. For a potential structure with \\(p\\) variables, a thorough search of all possible networks requires \\(2^{p(p-1)/2}\\) calculations (you can draw \\(p(p-1)/2\\) lines in a network, and two possible directions for each line). To reduce computational expense, a heuristic search is necessary. Hill-climbing (HC) is one of the most popular learning approaches in artificial intelligence, which employs a recursive process that seeks the local optimum at each iteration. In the Bayesian network learning setting, HC starts with a random possible network and tries all modified networks by a single edge change: addition, deletion, or reversal. HC chooses the model with the highest score of all modified networks and then repeats the iterative process unless no modified networks could increase the score. A toy example of one HC iteration is shown below.\n\n\n\nAssume we have three predictors. We start with a random initial network shown above, which is characterized by a reverse v-structure(which will be explained later), where A is the parent node of two conditionally independent leaf nodes B and C. To perform one entire HC iteration, we list all possible networks after one edge modification.\nReversal:\n\n\n\nAddition:\n\n\n\nAnd deletion:\n\n\n\nWe then record the chosen score for each of the six new modified networks and the original initial state. If any modified network can improve the score, we choose the new network with the best score and start another iteration; if no modification improves the score, we settle for the old network.\nAs a heuristic search, HC does not guarantee to find the global best network and only seeks the local optimum. As the initial starting network is random, the resulting network structure might be slightly different each time you run HC function.\n2.2 Constraint-based algorithms\nThe constraint-based approach to network learning was established by the seminal work of Verma and Pearl(1991). The algorithm is roughly summarized in three steps:\nLearning neighbors. For every possible pairs of nodes \\(X_i\\) and \\(X_j\\)(\\(i\\neq j\\)), search for a separating set \\(S_{ij}\\) such that given \\(S_{ij}\\)(including \\(\\emptyset\\)), \\(X_i\\) and \\(X_j\\) are conditionally independent. If such a set does not exist, an undirected arc is placed between \\(X_i\\) and \\(X_j\\). After this step is done, we check for neighbor symmetry. That is, if \\(X_i\\) is \\(X_j\\)’s neighbor, then \\(X_j\\) must also be \\(X_i\\)’s neighbor. If this is not the case, we drop the arc.\nExamining v-structure. A V-structure is defined by a triplet of nodes, incident on a converging connection, shown in the example below.\n\n\n\nFor each pair of non-adjacent nodes \\(X_i\\) and \\(X_j\\) that has a common neighbor \\(X_k\\), if \\(X_k\\) is in the separating set \\(S_{ij}\\), then we set the direction of arcs \\(X_i \\rightarrow X_k\\) and \\(X_i \\rightarrow X_k\\) to obtain a v-structure.\nCompleting the DAG. This step calls for a logical deduction of arc direction for those undirected arcs to satisfy the acyclicity requirement. Scutari(2020) outlines the recursive steps: a. If \\(X_i\\) and \\(X_j\\) are adjacent, and if there is a strict directed path from \\(X_i\\) to \\(X_j\\), then place \\(X_i\\rightarrow X_j\\). b. If \\(X_i\\) and \\(X_j\\) are not adjacent, but \\(X_i\\rightarrow X_k\\) and there’s an undirected arc from \\(X_k\\) to \\(X_j\\), then set \\(X_k\\rightarrow X_j\\).\nThe network learned by constraint-based algorithms may contain some undirected arcs. Central to constraint-based network learning is the concept of conditional independence test and the Markov blanket. \\(X_i\\) and \\(X_j\\) are conditionally independent given \\(X_k\\), denoted by \\(X_i ⊥ X_j | X_k\\), if and only if \\(P(X_i,X_j|X_k)=P(X_i|X_k)P(X_j|X_k)\\). The most popular conditional independence tests are mutual information tests (for discrete) and exact Student’s t-tests.\nMarkov blanket is often used to reduce computational expense. From the summarized outline of constraint-based algorithms, we can see that it is the first step that asks for a search of every possible separating set for every possible pair of nodes that contains most of the computing process. To effectively reduce this process, we often limit our search of separating sets only among the Markov blanket of the two nodes. A Markov blanket of a variable \\(X_i\\), \\(\\mathbb{B}(X_i)\\), refers to the subset of variables in \\(\\mathbb{V}\\) that graphically separates \\(X_i\\) from any other nodes, or in notation, \\(X_i ⊥ \\mathbb{V}\\backslash\\{X_i,\\mathbb{B}(X_i)\\} | \\mathbb{B}(X_i)\\). In Bayesian network specifically, the Markov blanket is identified by the parents, children, and spouses. In the example graph below, for node E, the Markov blanket contains: its parents, B and C; its children or immediate descendants, G; and its spouse, F, with whom it shares a common children node, G. A and D are part of the Markov blanket of E. Markov blankets could greatly improve the efficiency of the algorithms as not all variables are entirely relevant in making statistical inferences.\n\n\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nBecause Bayesian network works best with discrete data, we categorize our quantitative continuous variables into 3 categories by using quantile. Data wrangling process will be shown in the appendix.\nWe have 11 categorical variables in total\n5 of them are Boolean variables(HolidaySeason/producer_power/director_power/distributor_power/genre_power)\n3 of them with 3 levels which are High, Medium and Low.(domestic_cut/budget_cut/numVotes_cut)\nA_power has 5 categories(0/1/2/3/4) where representing how many influential artists appeared in the main cast of 4\nmpaa has 6 categories(1/2/3/4/5/NA) which includes (insert here)\naverageRating_round has total 7 levels coresponding with the rounded rating score\nIncorporating prior knowledge\nFollowing Lee and Chang(2009), we incorporate our prior knowledge of the system under study into the model selection process. The prior knowledge is shown in the graph below with certain modifications.\n From Lee and Chang(2009)\nWe will split our 11 categorical variables into four blocks:\nBox-office performance(domestic_cut)\nProduction(mpaa/budget_cut/A_power/director_power/producer_power/genre_power)\nDistribution(distributor_power/HolidaySeason)\nExhibition (numVotes_cut/averageRating_round)\nWe will restrict the model selection process by blacklisting arcs that point from a later to an earlier block.\nBox office revenue cannot influence production, distribution and exhibition\nProduction can only influence Distribution and Exhibition\nDistribution can only influence exhibition\n\n\n## Incorporate subjective prior \n\n# Box office revenue cannot influence production, distribution and exhibition \nblacklist_1 = tiers2blacklist(list(c(\"mpaa\", \"budget_cut\", \"A_power\", \"director_power\", \"producer_power\", \"genre_power\", \"distributor_power\", \"HolidaySeason\", \"numVotes_cut\", \"averageRating_round\"), \"domestic_cut\"))\n\n#Distribution can only influence exhibition \nblacklist_2 = tiers2blacklist(list(c(\"numVotes_cut\", \"averageRating_round\"), c(\"distributor_power\", \"HolidaySeason\")))\n\n#Production can only influence Distribution and Exhibition \nblacklist_3 = tiers2blacklist(list(c(\"mpaa\", \"budget_cut\", \"A_power\", \"director_power\", \"producer_power\", \"genre_power\"), c(\"numVotes_cut\", \"averageRating_round\", \"distributor_power\", \"HolidaySeason\")))\n\nbl = rbind(blacklist_1, blacklist_2, blacklist_3)\n\n\n\nHill climbing algorithm\nWe chose to use Hill Climbing algorithm in our analysis.\nNetwork 1: Without prior Knowledge Without Blacklisting arcs\n\n\nbn_net0 <- hc(imdb_movies_discrete)\n\n\n\n\n\ngraphviz.plot(bn_net0)\nna = narcs(bn_net0)\n# Check out the network\nbn_net0\n\n\n\n\nNetwork exploration\n\n\n# Check out the Arc Strength \narc.strength(bn_net0,imdb_movies_discrete) %>% \n  arrange(desc(strength))\n\n\n            from                  to    strength\n1   numVotes_cut      director_power   -3.815918\n2     budget_cut             A_power  -10.690161\n3        A_power      producer_power  -22.731520\n4   numVotes_cut             A_power  -26.889533\n5           mpaa        numVotes_cut  -32.786270\n6  HolidaySeason        domestic_cut  -33.785634\n7     budget_cut         genre_power  -51.000243\n8     budget_cut                mpaa  -86.250768\n9     budget_cut   distributor_power -124.445162\n10  numVotes_cut averageRating_round -407.358472\n11  domestic_cut          budget_cut -429.056293\n12  domestic_cut        numVotes_cut -627.306396\n\nAs the HC algorithm has a random initial model, the HC function in the bnlearn package might produce a different network each time it runs. To test for network stability, we run the algorithm 1000 times and see how the models differ from each other. This is shown in the graph below. The stronger the arc color is, the more often such arc is existent in the bootstrap network samples.\n\n\n#checking for stability by bootstrapping\nboot<-boot.strength(imdb_movies_discrete,R=1000, algorithm = \"hc\")\nqgraph(boot)\n\n\n\n\nThis network is not ideal, as the only root node is HolidaySeason, which is also the only parent node to our intended outcome is domestic_cut. As we expected, there is a lot of reverse causality within the variables. To make box office prediction the primary goal, we potentially need to blacklist some relationships and reverse some arc directions manually.\nNetwork 2: With Prior Knowledge With Blacklisting arcs\n\n\nbn_hc <- hc(imdb_movies_discrete, blacklist=bl)\n\n\n\n\n\ngraphviz.plot(bn_hc)\nna = narcs(bn_hc)\nbn_hc\n\n\n\n\nThis network has 11 nodes and 12 directed arcs. There are 139 tests used in the learning procedure. As a modification from the network without blacklisting, domestic cut is now one of the leaf nodes with two immediate parents: budget cut and numVotes_cut.\nCompare two Bayesian Networks\n\n\n# compare score based algorithms\nall.equal(bn_net0, bn_hc)\n\n\n[1] \"Different arc sets\"\n\nunlist( bnlearn::compare(bn_net0, bn_hc) )\n\n\ntp fp fn \n 5  7  7 \n\nThese two networks contain different arc sets. Five arcs appear in the DAG learned by the Hill-Climbing algorithm, both with and without blacklist. Additionally, seven arcs appear in the DAG with blacklist, but not in the one without, and vice-versa.\n\n\npar(mfrow = c(1, 2))\ngraphviz.compare(bn_net0, bn_hc)\n\n\n\n\nWe can see that the new network with blacklist make more sense intuitively compared with the last one.\nExplaination of Bayesian information criterion\nBayesian information criterion (BIC) is a criterion for model selection among a finite set of models.\nBIC attempts to resolve the problem of overfitting by introducing a penalty term for the number of parameters.\nModel with lower BIC is preferred.\n\n\ns1 <- score(bn_net0, data = imdb_movies_discrete, type = \"bic\")\ns2 <- score(bn_hc, data = imdb_movies_discrete, type = \"bic\")\nc(s1, s2)\n\n\n[1] -22548.04 -22600.90\n\nBased on the network score, the DAG produced with the network with blacklist provides a slightly better fit to the data.\nFitting a BN model and Parameter estimate\nThe joint probability distribution of the model obtained with the hill climbing algorithm factorized according to:\nP(A_power, producer_power, budget_cut, mpaa, HolidaySeason,director_power, distributor_power, genre_power, numVotes_cut, avereageRating_round, domestic_cut) = P(A_power)(producer_power|A_power)(budget_cut|A_power)(mpaa|budget_cut)(HolidaySeason|budget_cut)(director_power|producer_power)(distributor_power|budget_cut)(genre_power|budget_cut)(numVotes_cut|A_power, budget_cut)(domestic_cut|budget_cut,numVotes_cut)(averageRating_round|numVotes_cut)\n\n\n# Fit the model \nbn_model = bn.fit(bn_hc, imdb_movies_discrete) \n\n# Check out the model coefficient for domestic_cut\ncoef(bn_model$domestic_cut)\n\n\n, , numVotes_cut = low\n\n            budget_cut\ndomestic_cut         low      medium        high\n      low    0.686520376 0.522988506 0.333333333\n      medium 0.310344828 0.465517241 0.615384615\n      high   0.003134796 0.011494253 0.051282051\n\n, , numVotes_cut = medium\n\n            budget_cut\ndomestic_cut         low      medium        high\n      low    0.420382166 0.167095116 0.028301887\n      medium 0.535031847 0.697943445 0.578616352\n      high   0.044585987 0.134961440 0.393081761\n\n, , numVotes_cut = high\n\n            budget_cut\ndomestic_cut         low      medium        high\n      low    0.139784946 0.029850746 0.000000000\n      medium 0.612903226 0.488805970 0.122093023\n      high   0.247311828 0.481343284 0.877906977\n\ndomestic_cut is directly influenced by budget_cut and numVotes_cut. The coefficients represent the conditional probability distribution of domestic_cut. P(domestic_cut|budget_cut,numVotes_cut)\nFor example, in the third matrix, the coefficient “0.1398” is the conditional probability of domestic_cut being low, given high numVotes_cut and low budget_cut.\nMaking predictions of box office\nAs all variables are interconnected, once any specific information is given (and becomes our new prior), it could further impact all other nodes. Bayesian networks could be effectively used for prediction purposes.\nIn the first example shown below, given that a movie has strong director power and is released in holiday seasons, the Bayesian network predicts that the movie will have a 36% probability of being in the fourth quantile of the domestic box office.\nIn the second example shown below, given that we have high domestic box office, non-influential producer, and the holiday season release, the Bayesian network predicts that this movie is unlikely to have cast four influential actors/actresses.\n\n\nset.seed(84735)\n#predicting the domestic_cut\ncpquery(bn_model,event=(domestic_cut==\"high\"),evidence=(director_power==TRUE& HolidaySeason==TRUE))\n\n\n[1] 0.3673469\n\n#predicting Apower\ncpquery(bn_model,event=(A_power==4),evidence=( domestic_cut==\"high\"&producer_power==FALSE&HolidaySeason==TRUE))\n\n\n[1] 0.01098901\n\n\nAppendix\nReferences\nAntal, P., Fannes, G., Timmerman, D., Moreau, Y., & De Moor, B. P. (2004). Using literature and data to learn Bayesian networks as clinical models of ovarian tumors. Artificial Intelligence in Medicine, 30, 257–281.\nBessler, D (2000). d-separation without Tears. Texas A&M University. Retrived from http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html\nBrufatto, Verena. “Bayesian-Networks/bayesian_networks.RMD at Main · Vettorefburana/Bayesian-Networks.” GitHub, 30 Nov. 2021, https://github.com/vettorefburana/Bayesian-Networks/blob/main/bayesian_networks.Rmd.\nChillar, Arush. “Bayesian Structure Learning.” Kaggle, Kaggle, 1 June 2021, https://www.kaggle.com/arushchillar/bayesian-structure-learning/notebook.\nElashkar, E. (2016). “Constraint Based Bayesian Network Structure Learning Algorithms, Using BNLearn R Package.” https://www.youtube.com/watch?v=4JkddqxGrO0\nJohnson, A. A., Ott, M. Q., and Dogucu, M. (2021) Bayes Rules! An Introduction to Applied Bayesian Modeling. Retrieved from https://www.bayesrulesbook.com/\nKirko, Igor. “Boxofficemojo Dataset.” Kaggle, 26 Apr. 2020, https://www.kaggle.com/igorkirko/wwwboxofficemojocom-movies-with-budget-listed.\nKorb K, Nicholson A (2004). Bayesian Artificial Intelligence. Chapman and Hall.\nKuleshov, V., Ermon, S. Structure Learning for Bayesian Networks. Stanford University. Retrieved from https://ermongroup.github.io/cs228-notes/learning/structure/\nMargaritis D (2003). “Learning Bayesian Network Model Structure from Data.” Ph.D. thesis, School of Computer Science, Carnegie-Mellon University, Pittsburgh, PA. Available as Technical Report CMU-CS-03-153.\nScutari, M (2020). “Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimized Implementations in the bnlearn R Package.” Journal of Statistical Software, 77(2), DOI:10.18637/jss.v077.i02\nVerma TS, Pearl J (1991). “Equivalence and Synthesis of Causal Models.” Uncertainty in Artificial Intelligence, 6, 255–268.\nCode Book\nvariable\ndescription\ntitle\ntitle (name) of the movie\nyear\nyear of production(from 1990 to 2020)\nmpaa_factor\nThe MPAA rating of a movie, 6 categories:\n\n(G, PG, PG-13, R, NC-17, NA)\nbudget\nthe budget of a movie in millions (USD)\ndomestic\nthe box office revenue of a movie in millions within the United States\ndirector_power\nTrue: The director is in the top 100 Grossing Director at the Worldwide Box Office\n\nFalse: otherwise\nproducer_power\nTrue:the producer is in the top 100 Grossing Producer at the Worldwide Box Office\n\nFalse: otherwise\nA_power\nThe count of the four most popular actors within a movies cast with a previous box-office success (0, 1, 2, 3, 4).\ngenre_power\nTrue: the primary genre of the movie is Adventure/Action/Drama/Comedy\n\nFalse: otherwise\ndistributor_power\nTrue: The distributor of this movie is one of the top 10 distributors (measured in total box office revenue from 1995 to 2021)\n\nFalse: otherwise\nHolidaySeason\nTrue: April, June, July, November, December\n\nFalse: otherwise\nnumVotes\nnumber of votes the title has received on IMDB.com\naverageRating\nweighted average of all individual user ratings\ndomestic_cut\nthe box office revenue of a movie in millions within the United States cut into three groups: Low (Quartile 1), Medium (Quartiles 2-3), High (Quartile 4)\nbudget_cut\nthe budget of a movie in millions (USD) cut into three groups: Low (Quartile 1), Medium (Quartiles 2-3), High (Quartile 4)\nnumVotes_cut\nthe number of votes the title has received cut into three groups: Low (Quartile 1), Medium (Quartiles 2-3), High (Quartile 4)\naverageRating_round\nweighted average of all individual user ratings rounded to the nearest integer\nData Cleaning\n\n\n# Load movies data\nmovies <- read.csv(\"/Users/alexmccreight/BayesianStats/Bayesian Project/Raw Data/Mojo_budget_update.csv\")\n\n# Load ratings data\nratings <- read.csv(\"/Users/alexmccreight/BayesianStats/Bayesian Project/Raw Data/rating.csv\")\n\n# Drop trivia, writer, composer, cinematographer, international, and html columns\nmovies_new <-\n  movies %>% \n  select(-c(trivia, writer, composer, cinematographer, international, html))\n\n# Implement NA's into blank data\nmovies_new <- movies_new                                   \nmovies_new[movies_new == \"\" | movies_new == \" \"] <- NA \n\n\n\nSeperate the date variable into 2 different variables\n\n\nsplit_date <- tibble(date = movies_new$release_date)\nsplit_date <- separate(split_date, date, c(\"release_month\", \"realease_day\"))\n\nmovies_new <-\n  cbind(movies_new, split_date)\n\n\n\nChange runtime to a quantative variable\n\n\nmovies_new <- movies_new %>% \n  mutate(run_time_min = format(as.POSIXct(run_time, format = '%H hr %M min'), format = '%M')) %>% \n  mutate(run_time_hour = format(as.POSIXct(run_time, format = '%H hr %M min'), format = '%H')) %>% \n  mutate(run_time_total_min = as.integer(run_time_hour)*60 + as.integer(run_time_min)) %>% \n  select(-c(run_time_min, run_time_hour, run_time))\n\n\n\nAdd a Holiday Season Variable\n\n\n# Create a variable to determine whether or not a movie was released during the holiday season (We chose Easter, Summer Holidays, Thanksgiving, and Christmas)\nmovies_new <- movies_new %>% \n  mutate(HolidaySeason = release_month %in% c(\"April\", \"June\", \"July\", \"November\", \"December\"))\n\n\n\nAdd stars power vairable\n\n\n# Create a variable to determine whether or not the cast is in the top 100 stars in leading roles at the worldwide box office\n\nstars <- read_html(\"https://www.the-numbers.com/box-office-star-records/worldwide/lifetime-acting/top-grossing-leading-stars\")\n\n# Retrive and inspect Rank\ntop_100_rank <- \n  stars %>%\n  html_nodes(\"#page_filling_chart .data\") %>%\n  html_text()\n\n# Retrive and inspect Name\ntop_100_name <- \n  stars %>%\n  html_nodes(\"#page_filling_chart b a\") %>%\n  html_text()\n\n# Retrive and inspect Worldwide Box Office\ntop_100_worldwide_box_office <- \n  stars %>%\n  html_nodes(\"td:nth-child(3)\") %>%\n  html_text()\n\n# Retrive and inspect number of Movies\ntop_100_num_Movies <- \n  stars %>%\n  html_nodes(\"td:nth-child(4)\") %>%\n  html_text()\n\n# Retrive and inspect Average box office per movie\ntop_100_avg_box_office <- \n  stars %>%\n  html_nodes(\"td:nth-child(5)\") %>%\n  html_text()\n\ntop_100_actor <- tibble(Rank = top_100_rank, Name = top_100_name, `Worldwide Box Office` = top_100_worldwide_box_office, Movies = top_100_num_Movies, Average = top_100_avg_box_office)\n\n# Add new variables \nmovies_new <- movies_new %>% \n  mutate(A_power_1 = main_actor_1 %in% top_100_actor$Name)\nmovies_new <- movies_new %>% \n  mutate(A_power_2 = main_actor_2 %in% top_100_actor$Name)\nmovies_new <- movies_new %>% \n  mutate(A_power_3 = main_actor_3 %in% top_100_actor$Name)\nmovies_new <- movies_new %>% \n  mutate(A_power_4 = main_actor_4 %in% top_100_actor$Name)\n\n# Combine the Actors power by adding the 4 main actors/actress together\n\nmovies_new <- movies_new %>% \n  mutate(A_power = A_power_1 + A_power_2 + A_power_3 + A_power_4)\n\n\n\nAdd a director power vairable\n\n\n# Create a variable to determine whether or not the direct is in the top 100 Grossing Director at the Worldwide Box Office\n\ndirectors <- read_html(\"https://www.the-numbers.com/box-office-star-records/worldwide/lifetime-specific-technical-role/director\")\n\n# Retrive and inspect Name\ntop_100_Director_Name <- \n  stars %>%\n  html_nodes(\"#page_filling_chart b a\") %>%\n  html_text()\n\n# Add new variable\nmovies_new <- movies_new %>% \n  mutate(director_power = director %in% top_100_Director_Name)\n\n\n\nAdd a producer power vairable\n\n\n# Create a variable to determine whether or not the producer is in the top 100 Grossing Producer at the Worldwide Box Office\n\ndirectors <- read_html(\"https://www.the-numbers.com/box-office-star-records/worldwide/lifetime-specific-technical-role/producer\")\n\n# Retrive and inspect Name\ntop_100_Producer_Name <- \n  stars %>%\n  html_nodes(\".data+ td\") %>%\n  html_text()\n\n# Add new variable\nmovies_new <- movies_new %>% \n  mutate(producer_power = producer %in% top_100_Producer_Name)\n\n\n\nAdd a power of the distributor variable\n\n\n# Create a variable to determine the power of a distributor (1 = top 10 distributor, 0 = otherwise)\n\ndistributor_1 <- c(\"Walt Disney Studios Motion Pictures\", \"Warner Bros.\", \"Sony Pictures Releasing\", \"Universal Pictures\", \"Twentieth Century Fox\", \"Paramount Classics\", \"Lionsgate\", \"New Line Cinema\", \"DreamWorks\", \"Miramax\")\n\n\n# Add new variable\nmovies_new <- movies_new %>% \n  mutate(distributor_power = distributor %in% distributor_1)\n\n\n\nGenre\n\n\n# We kept the primary genre of each movie. Based on the market share for box office revenue, we decided to make a dummy variable, genre_power, that includes Adventure, Action, Drama, and Comedy, as they account for 77.03% of all revenues in the box office from 1995 to 2021. \n\n# https://www.the-numbers.com/market/genres\n\nmovies_new <- movies_new %>% \n  mutate(genre_power =  genre_1 %in% c(\"Adventure\", \"Action\", \"Drama\", \"Comedy\"))\n\n\n\nJoining the datasets\n\n\n# Join the movies dataset with the ratings dataset\nimdb_movies <- movies_new %>% \n  inner_join(ratings, by = c(\"movie_id\" = \"tconst\")) \n\n\n\nChange Quantitative Variables into Categorical Variables\n\n\nimdb_movies_new <- read.csv(\"/Users/alexmccreight/BayesianStats/Bayesian Project/imdb_movies_new.csv\")\nimdb_movies_new <- \n  imdb_movies_new %>% \n  mutate(domestic_cut = cut(domestic, breaks= c(0, 17600423,  84351197, Inf), \n                                labels = c(\"low\", \"medium\", \"high\"))) %>% \n  mutate(budget_cut = cut(budget, breaks= c(0, 18000000,  69000000, Inf), \n                                labels = c(\"low\", \"medium\", \"high\"))) %>% \n  mutate(numVotes_cut = cut(numVotes, breaks= c(0, 36227,  202107, Inf), \n                                labels = c(\"low\", \"medium\", \"high\"))) %>% \n  mutate(averageRating_round = round(averageRating))\n\n\n\n\n\n\n\n",
    "preview": "posts/BayesCapstone/BayesCapstone_post_files/figure-html5/data-viz-1-1.png",
    "last_modified": "2021-12-16T17:16:32-06:00",
    "input_file": {}
  },
  {
    "path": "posts/LinearFinalProject/",
    "title": "Macalester Department Cross-Listed Centrality Analysis",
    "description": "Found the degree and eigenvalue centrality to measure the connectedness of the departments at macalester.",
    "author": [],
    "date": "2020-10-10",
    "categories": [],
    "contents": "\n\n\nrequire(pracma)\nlibrary(igraph)\n\n\n\n\n\ndepartment<-read.csv(\"/Users/alexmccreight/Desktop/Misc./cs3.csv\")  # read in the data\n\n\n\n\n\nDepartmentName = c(\"American Studies\", \"Anthropology\", \"Art and Art History\", \"Asian Studies\", \"Biology\", \"Chemistry\", \"Chinese\", \"Classical Mediterranean and Middle East\", \"Computer Science\", \"Economics\", \"Educational Studies\", \"English\", \"Environmental Studies\", \"French and Francophone Studies\", \"Geography\", \"Geology\", \"German Studies\", \"History\", \"International Studies\", \"Interdisciplinary Studies\",\"Japanese\", \"Latin American Studies\", \"Linguistics\", \"Mathematics\", \"Media and Culutral Studies\", \"Music\", \"Neuroscience\", \"Physical Education\", \"Philosophy\", \"Physics and Astronomy\", \"Political Science\", \"Portugese\", \"Psychology\", \"Religious Studies\", \"Russian Studies\", \"Sociology\", \"Spanish\", \"Statistics\", \"Theater and Dance\", \"Women's, Gender, and Sexuality Studies\" )\n\n\n\n\n\nA<-as.matrix(department[,1:40]) # convert the departments to a matrix\n\ndim(A)\nhead(A)  # shows the top part of the matrix (first 6 rows)\n\n\n\n\n\n\nExplain what your network is: what are the vertices when are two connected by an edge\nOur network measures the amount of classes cross-listed between different departments at Macalester. The verticies are the department names, and the edges are if the departments share a class that is cross-listed.\nDetermine the degree centrality of each vertex.\n\n\nv=rep(1,nrow(A)) # all 1s vector\nd = A %*% v  # degrees\nu=d/sum(d)   # proportion of degrees\ncbind(d,u) # show d and u together side-by-side in a matrix\n\n\n                                        [,1]        [,2]\nAmerican Studies                          18 0.080000000\nAnthropology                               3 0.013333333\nArt and Art History                        4 0.017777778\nAsian Studies                             10 0.044444444\nBiology                                   10 0.044444444\nChemistry                                  3 0.013333333\nChinese                                    2 0.008888889\nClassical Mediterranean and Middle East    6 0.026666667\nComputer Science                           6 0.026666667\nEconomics                                  1 0.004444444\nEducational Studies                        2 0.008888889\nEnglish                                    6 0.026666667\nEnvironmental Studies                     24 0.106666667\nFrench And Francophone Studies             0 0.000000000\nGeography                                  4 0.017777778\nGeology                                    6 0.026666667\nGerman Studies                             4 0.017777778\nHistory                                   15 0.066666667\nInternational Studies                      6 0.026666667\nIntersisciplinary Studies                  0 0.000000000\nJapanese                                   3 0.013333333\nLatin American Studies                    12 0.053333333\nLinguistics                                2 0.008888889\nMathematics                                3 0.013333333\nMedia and Cultural Studies                 5 0.022222222\nMusic                                      1 0.004444444\nNeuroscience                               0 0.000000000\nPhysical Education                         0 0.000000000\nPhilosophy                                 5 0.022222222\nPhysics and Astronomy                      1 0.004444444\nPolitical Science                         13 0.057777778\nPortugese                                  2 0.008888889\nPsychology                                 6 0.026666667\nReligious Studies                          5 0.022222222\nRussian Studies                            3 0.013333333\nSociology                                  5 0.022222222\nSpanish                                    5 0.022222222\nStatisitics                                5 0.022222222\nTheater and Dance                          5 0.022222222\nWomen's, Gender, and Sexuality Studies    14 0.062222222\n\nDetermine the eigenvector centrality of each vertex in two ways:\n-Find the dominant eigenvector by looping and using Gould’s index.\n\n\n(B = A + diag(nrow(A)))\n\n\n\n\n\nN = 1000 \nw = rep(1,nrow(B))\nfor (i in 2:(N+1)) {  \n   w = A %*%  w\n   w = w/sum(w)\n}\nw\n\n\n                                               [,1]\nAmerican Studies                        0.082649618\nAnthropology                            0.002808020\nArt and Art History                     0.006432586\nAsian Studies                           0.026141263\nBiology                                 0.076833790\nChemistry                               0.020810529\nChinese                                 0.004720263\nClassical Mediterranean and Middle East 0.010530491\nComputer Science                        0.011799827\nEconomics                               0.011688650\nEducational Studies                     0.008382594\nEnglish                                 0.024040463\nEnvironmental Studies                   0.129465704\nFrench And Francophone Studies          0.000000000\nGeography                               0.037426081\nGeology                                 0.070131899\nGerman Studies                          0.024286781\nHistory                                 0.066139198\nInternational Studies                   0.018560317\nIntersisciplinary Studies               0.000000000\nJapanese                                0.006891253\nLatin American Studies                  0.059597324\nLinguistics                             0.002011360\nMathematics                             0.001697782\nMedia and Cultural Studies              0.024763528\nMusic                                   0.002170463\nNeuroscience                            0.000000000\nPhysical Education                      0.000000000\nPhilosophy                              0.021119962\nPhysics and Astronomy                   0.011688650\nPolitical Science                       0.093655804\nPortugese                               0.010761340\nPsychology                              0.010197584\nReligious Studies                       0.013705363\nRussian Studies                         0.013618285\nSociology                               0.032109360\nSpanish                                 0.019470184\nStatisitics                             0.003502562\nTheater and Dance                       0.016144825\nWomen's, Gender, and Sexuality Studies  0.024046298\n\n\n\nvecs = eigen(A)$vectors\nv1 = vecs[,1]\nv1 = v1/sum(v1)\ncbind(w,v1)\n\n\n                                                                   v1\nAmerican Studies                        0.082649618+0i 0.082649618+0i\nAnthropology                            0.002808020+0i 0.002808020+0i\nArt and Art History                     0.006432586+0i 0.006432586+0i\nAsian Studies                           0.026141263+0i 0.026141263+0i\nBiology                                 0.076833790+0i 0.076833790+0i\nChemistry                               0.020810529+0i 0.020810529+0i\nChinese                                 0.004720263+0i 0.004720263+0i\nClassical Mediterranean and Middle East 0.010530491+0i 0.010530491+0i\nComputer Science                        0.011799827+0i 0.011799827+0i\nEconomics                               0.011688650+0i 0.011688650+0i\nEducational Studies                     0.008382594+0i 0.008382594+0i\nEnglish                                 0.024040463+0i 0.024040463+0i\nEnvironmental Studies                   0.129465704+0i 0.129465704+0i\nFrench And Francophone Studies          0.000000000+0i 0.000000000+0i\nGeography                               0.037426081+0i 0.037426081+0i\nGeology                                 0.070131899+0i 0.070131899+0i\nGerman Studies                          0.024286781+0i 0.024286781+0i\nHistory                                 0.066139198+0i 0.066139198+0i\nInternational Studies                   0.018560317+0i 0.018560317+0i\nIntersisciplinary Studies               0.000000000+0i 0.000000000+0i\nJapanese                                0.006891253+0i 0.006891253+0i\nLatin American Studies                  0.059597324+0i 0.059597324+0i\nLinguistics                             0.002011360+0i 0.002011360+0i\nMathematics                             0.001697782+0i 0.001697782+0i\nMedia and Cultural Studies              0.024763528+0i 0.024763528+0i\nMusic                                   0.002170463+0i 0.002170463+0i\nNeuroscience                            0.000000000+0i 0.000000000+0i\nPhysical Education                      0.000000000+0i 0.000000000+0i\nPhilosophy                              0.021119962+0i 0.021119962+0i\nPhysics and Astronomy                   0.011688650+0i 0.011688650+0i\nPolitical Science                       0.093655804+0i 0.093655804+0i\nPortugese                               0.010761340+0i 0.010761340+0i\nPsychology                              0.010197584+0i 0.010197584+0i\nReligious Studies                       0.013705363+0i 0.013705363+0i\nRussian Studies                         0.013618285+0i 0.013618285+0i\nSociology                               0.032109360+0i 0.032109360+0i\nSpanish                                 0.019470184+0i 0.019470184+0i\nStatisitics                             0.003502562+0i 0.003502562+0i\nTheater and Dance                       0.016144825+0i 0.016144825+0i\nWomen's, Gender, and Sexuality Studies  0.024046298+0i 0.024046298+0i\n\n-Using the dominant eigenvector with the built-in eigen function.\n\n\neigen(B)\n\n\neigen() decomposition\n$values\n [1] 12.0761898 -9.0822074  9.0761005  6.4760123 -5.7095441  5.2652366\n [7]  4.7731800 -4.2527888  3.7955328  3.5935375  3.3262670 -3.2003402\n[13]  2.9780591  2.7250911 -2.6756693  2.3192413 -2.2904420  1.9854495\n[19] -1.5917559  1.5010394  1.3390334 -1.2299909  1.0742927 -1.0125394\n[25]  1.0000000  1.0000000  1.0000000  1.0000000  1.0000000  1.0000000\n[31]  1.0000000  1.0000000  1.0000000  0.8785235 -0.8197609  0.5833805\n[37] -0.5200265 -0.3406748 -0.2242584  0.1838315\n\n$vectors\n             [,1]         [,2]         [,3]         [,4]\n [1,] 0.338038051  0.017318816 -0.414284089  0.112227509\n [2,] 0.011484841 -0.004866108 -0.029339889  0.007804564\n [3,] 0.026309363 -0.008455336 -0.069835679 -0.163228345\n [4,] 0.106918117  0.062130335 -0.190605691 -0.163510376\n [5,] 0.314251234 -0.437096061  0.319869708 -0.063203937\n [6,] 0.085115344  0.130059631  0.118820849 -0.034625892\n [7,] 0.019305938 -0.012324748 -0.047202407 -0.059718776\n [8,] 0.043069849  0.009321333 -0.087248089 -0.065714083\n [9,] 0.048261450  0.048218634  0.035385023 -0.044508303\n[10,] 0.047806735 -0.065602294  0.045225051 -0.006017748\n[11,] 0.034284923 -0.001572591 -0.065504107 -0.063639072\n[12,] 0.098325819 -0.074486366 -0.040638998 -0.154698052\n[13,] 0.529516470  0.661415939  0.365242058 -0.032953259\n[14,] 0.000000000  0.000000000  0.000000000  0.000000000\n[15,] 0.153073174 -0.202969257  0.112073949 -0.047912631\n[16,] 0.286840410 -0.393613765  0.271350306 -0.036106485\n[17,] 0.099333262 -0.043671609  0.007627147  0.054287059\n[18,] 0.270510209 -0.123091668 -0.381541767  0.124089876\n[19,] 0.075911945  0.036615839 -0.130005663  0.086548349\n[20,] 0.000000000  0.000000000  0.000000000  0.000000000\n[21,] 0.028185317 -0.012768644 -0.071830393 -0.169087026\n[22,] 0.243753856  0.099914382 -0.324615233  0.262228461\n[23,] 0.008226489  0.003123939 -0.019698138  0.021903624\n[24,] 0.006943950 -0.002015736  0.008135503 -0.019655348\n[25,] 0.101283161  0.009473105 -0.164991564  0.089368007\n[26,] 0.008877224  0.007387903 -0.005032007 -0.028250129\n[27,] 0.000000000  0.000000000  0.000000000  0.000000000\n[28,] 0.000000000  0.000000000  0.000000000  0.000000000\n[29,] 0.086380929  0.055343455 -0.031911323  0.056953943\n[30,] 0.047806735 -0.065602294  0.045225051 -0.006017748\n[31,] 0.383053499 -0.285926275 -0.106741566  0.183907911\n[32,] 0.044014027 -0.019819942 -0.080389102  0.095773511\n[33,] 0.041708258 -0.001463630 -0.114733662 -0.460715851\n[34,] 0.056055117 -0.004454618 -0.122160002 -0.222242820\n[35,] 0.055698970  0.020785874 -0.110584210  0.061126251\n[36,] 0.131327714 -0.148378016  0.018060394  0.067461251\n[37,] 0.079633314 -0.026630096 -0.129744258  0.112139951\n[38,] 0.014325527 -0.013947782  0.015159058 -0.031562311\n[39,] 0.066032550  0.012170253 -0.110617528 -0.254742371\n[40,] 0.098349683  0.004475452 -0.198898094 -0.598901880\n               [,5]         [,6]         [,7]          [,8]\n [1,]  0.5125538819  0.148893301 -0.092192718  0.0144693192\n [2,] -0.0463632795 -0.070513894  0.069346766 -0.0381061892\n [3,] -0.0658148558 -0.146611848  0.030175435 -0.1845260974\n [4,]  0.3407464446 -0.463595550  0.183880753 -0.0109543724\n [5,]  0.1267173241 -0.008462940  0.104586861  0.0184364508\n [6,] -0.0566583908 -0.005952499  0.083155478 -0.0105295215\n [7,] -0.1015706697 -0.217383274  0.097467258  0.0041708787\n [8,]  0.1134789394 -0.107570924  0.056166758  0.1564433102\n [9,] -0.0269670880  0.213411150  0.421654425  0.0258868579\n[10,]  0.0162274616 -0.009051866 -0.012213209  0.0028918313\n[11,] -0.0916702564  0.052412618 -0.088036067  0.1013361389\n[12,] -0.0338196310  0.122701912 -0.006739571 -0.0930575864\n[13,] -0.1088788693 -0.038608349 -0.046082635 -0.0151901793\n[14,]  0.0000000000  0.000000000  0.000000000  0.0000000000\n[15,] -0.0021029501 -0.135847234  0.012094002  0.0107609334\n[16,]  0.0973647695 -0.054311193 -0.073279254  0.0173509881\n[17,]  0.0652868782  0.103928835 -0.064869133  0.0116763108\n[18,] -0.5745097937 -0.301265616  0.180231053 -0.0662100869\n[19,]  0.1764351302 -0.225616503  0.171191589  0.0316741449\n[20,]  0.0000000000  0.000000000  0.000000000  0.0000000000\n[21,] -0.0827740655 -0.204862708  0.049137488 -0.1228748478\n[22,]  0.0674314357  0.247990495 -0.068550594  0.0399064793\n[23,]  0.0211623997  0.032428985  0.034299480  0.0120463082\n[24,]  0.0004664866  0.154360089  0.402554466  0.0008199018\n[25,] -0.1031843545 -0.029345777  0.027077445 -0.0011572914\n[26,]  0.0050405259  0.028767903 -0.001786178  0.0177158439\n[27,]  0.0000000000  0.000000000  0.000000000  0.0000000000\n[28,]  0.0000000000  0.000000000  0.000000000  0.0000000000\n[29,]  0.0637355111  0.196678838 -0.006679750 -0.0319833343\n[30,]  0.0162274616 -0.009051866 -0.012213209  0.0028918313\n[31,] -0.2897174764  0.314556361 -0.219077973 -0.0130023895\n[32,] -0.0201001542  0.116284519 -0.036335714 -0.0151943971\n[33,]  0.1025117476  0.074658916 -0.239983203 -0.5467666536\n[34,] -0.0726246765  0.009508923 -0.091321656 -0.3164118607\n[35,]  0.1449553711 -0.194162203  0.140903350  0.0191795317\n[36,] -0.0301874396 -0.065754666  0.048147005 -0.0138734706\n[37,] -0.0956267749  0.208831187  0.060071346 -0.0251705235\n[38,]  0.0119185878  0.222485576  0.548628009 -0.0150968144\n[39,] -0.0287174995  0.117485505 -0.124665589 -0.2214143626\n[40,] -0.1261166449  0.053403177 -0.182356920  0.6673443675\n              [,9]        [,10]        [,11]        [,12]\n [1,] -0.059593648  0.004872839 -0.016709533 -0.411212537\n [2,]  0.233962060 -0.308289297  0.018290080 -0.056973922\n [3,]  0.344065568 -0.089344008 -0.192177235 -0.124294341\n [4,] -0.124819685  0.355593931 -0.090145651  0.383935195\n [5,] -0.057815768 -0.124066602 -0.331596653  0.060532759\n [6,] -0.062044452 -0.143510475 -0.427633603 -0.043234182\n [7,] -0.089299388  0.274215372 -0.077502410 -0.182811474\n [8,]  0.568872775 -0.285896751 -0.195309529  0.129371330\n [9,] -0.043703662  0.009440968  0.012154692 -0.018887811\n[10,]  0.004066675  0.006382471  0.035777187  0.004192828\n[11,] -0.063785526 -0.010458614  0.028129916  0.079578503\n[12,] -0.116622110  0.003662333  0.129436434  0.118466509\n[13,]  0.011368524  0.016553178  0.083227292 -0.017611304\n[14,]  0.000000000  0.000000000  0.000000000  0.000000000\n[15,] -0.032449669  0.156255099  0.068580357 -0.078827253\n[16,]  0.024400050  0.038294826  0.214663124  0.025156968\n[17,]  0.213168668  0.027673665  0.141486135 -0.085387991\n[18,] -0.071911038 -0.071347549  0.053737734 -0.205038411\n[19,]  0.023237891 -0.427802503  0.341129005  0.064370950\n[20,]  0.000000000  0.000000000  0.000000000  0.000000000\n[21,] -0.107571135  0.268232244 -0.063011752 -0.154099673\n[22,] -0.064321951  0.146785344 -0.246789250  0.474582608\n[23,]  0.061937956 -0.085860610 -0.103271865  0.045567577\n[24,] -0.100759268  0.029757266  0.071698116 -0.002491533\n[25,]  0.016207367 -0.178031242  0.216197796  0.249617999\n[26,] -0.041717310  0.001412099  0.055641262 -0.028204027\n[27,]  0.000000000  0.000000000  0.000000000  0.000000000\n[28,]  0.000000000  0.000000000  0.000000000  0.000000000\n[29,]  0.390994304  0.047396889  0.004102887 -0.126525600\n[30,]  0.004066675  0.006382471  0.035777187  0.004192828\n[31,]  0.177349818  0.185853865  0.025606557  0.253177521\n[32,] -0.046017668  0.113193152 -0.212176199 -0.225973412\n[33,] -0.118720885 -0.031997647  0.082147230  0.076955746\n[34,]  0.349126059 -0.230555984 -0.146118353  0.093723012\n[35,] -0.043134598 -0.219968901  0.192843068  0.082304254\n[36,]  0.001749533 -0.260537315  0.258750750 -0.135251400\n[37,] -0.060812470  0.085606582 -0.258528018 -0.134425407\n[38,] -0.118986091  0.033867809  0.077317137  0.014676549\n[39,] -0.141295571 -0.007263219  0.133080859  0.098915363\n[40,] -0.051079270 -0.015517467  0.033709141 -0.120599331\n              [,13]        [,14]        [,15]        [,16]\n [1,]  0.0657600213 -0.119016835 -0.266876909 -0.165166353\n [2,] -0.1875662674 -0.035128542 -0.070975301  0.284013783\n [3,] -0.1667860877 -0.154780040 -0.221668380 -0.173432414\n [4,]  0.0736850629  0.011828406  0.380708937  0.099420448\n [5,]  0.2323340986  0.033759638 -0.080060215  0.024777788\n [6,]  0.3523667647  0.058709312  0.065343376  0.056345543\n [7,]  0.0745023865  0.013713370 -0.207150808  0.150723679\n [8,] -0.2014461113 -0.189874410  0.266987841 -0.193548885\n [9,]  0.0017499864 -0.005872384  0.218768200  0.045345019\n[10,] -0.0504939149 -0.010822336  0.005464921 -0.022954326\n[11,]  0.0665309161  0.203217611  0.050504252 -0.067866164\n[12,] -0.0011794347 -0.383738077  0.014385222  0.133183730\n[13,] -0.0998799493 -0.018669517 -0.020087242 -0.030282294\n[14,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[15,] -0.1142305514 -0.025610325 -0.087180642  0.006498861\n[16,] -0.3029634892 -0.064934019  0.032789525 -0.137725957\n[17,]  0.3122388044  0.263444660 -0.076184329  0.322395788\n[18,]  0.0433931661 -0.023986852 -0.072121034 -0.150980103\n[19,]  0.0581611084  0.187961736 -0.013953586  0.244621904\n[20,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[21,]  0.0741456814  0.072209259 -0.179970738  0.195353971\n[22,] -0.1799061984  0.034763341 -0.057298363 -0.007630782\n[23,] -0.2277321646 -0.058687263  0.007847484  0.323609684\n[24,] -0.1599583164  0.044298844  0.053465877 -0.146880321\n[25,]  0.2756809002  0.109783112  0.189356741  0.064965283\n[26,] -0.0005962586 -0.222445105 -0.003913633  0.100954794\n[27,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[28,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[29,]  0.2504222236  0.213257270 -0.376619363  0.287855423\n[30,] -0.0504939149 -0.010822336  0.005464921 -0.022954326\n[31,]  0.1914036430  0.150095180  0.487378267  0.102779415\n[32,] -0.1819017396  0.040303193  0.031177104 -0.011568440\n[33,]  0.0658420648  0.469585734  0.081239980  0.075634509\n[34,] -0.1711492664 -0.172132457 -0.018306562 -0.329361950\n[35,]  0.0732776076  0.081148195  0.043038598 -0.043463091\n[36,] -0.1331324611  0.216422062  0.034110799  0.319159542\n[37,] -0.2629014199 -0.066112334  0.042130544  0.142905466\n[38,] -0.1590784973  0.041145964 -0.207645543 -0.119557800\n[39,]  0.0313387924 -0.396890052  0.119139218  0.165972183\n[40,] -0.0007055837  0.100910740 -0.099904952  0.058878123\n              [,17]        [,18]        [,19]        [,20]\n [1,] -0.0468650404  0.225872436 -0.088040283 -0.147260852\n [2,] -0.0981328167  0.149448904 -0.302485534 -0.107930529\n [3,] -0.1118007516 -0.008565450 -0.240272924 -0.036589588\n [4,] -0.0415456980  0.004044248 -0.167570758 -0.018755453\n [5,] -0.1706576401 -0.003342340  0.006597161 -0.007673675\n [6,]  0.1555939651 -0.010175073 -0.007636322 -0.045946538\n [7,]  0.0252523508  0.008207925  0.129310601 -0.074866187\n [8,]  0.2233057717  0.044185171  0.456809429  0.015042436\n [9,]  0.4864756117 -0.051869452 -0.168118080  0.111239248\n[10,]  0.0198412268  0.013378153 -0.011184748  0.007569450\n[11,]  0.0006161633  0.175758227 -0.056198266  0.110026815\n[12,] -0.1572200814 -0.053840555  0.154337844  0.086430987\n[13,] -0.0652864062  0.013183494  0.028988136  0.003792592\n[14,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[15,]  0.0721498557  0.044238421  0.031101057 -0.014724744\n[16,]  0.1190473605  0.080268917 -0.067108486  0.045416698\n[17,]  0.0736919562 -0.190979126  0.051491184  0.067456957\n[18,]  0.0299770347  0.080341763  0.051462843  0.163148981\n[19,]  0.0147393920 -0.220695505  0.228899588 -0.109207761\n[20,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[21,]  0.0365553777 -0.094136649  0.176891983 -0.134067608\n[22,]  0.0356808356 -0.165315619  0.016979833  0.030037422\n[23,]  0.0848551796  0.323784681  0.098259655  0.040087882\n[24,]  0.1930763443  0.119625185 -0.210874327 -0.192899048\n[25,] -0.0074999961  0.122189928 -0.060103287 -0.345528799\n[26,]  0.0477808393 -0.054635530 -0.059549529  0.172503390\n[27,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[28,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[29,] -0.3441362884 -0.283311046 -0.228992514  0.377766240\n[30,]  0.0198412268  0.013378153 -0.011184748  0.007569450\n[31,]  0.1744435817 -0.040262660  0.126655085 -0.002231444\n[32,] -0.0216875638 -0.335513123 -0.013102957  0.119900448\n[33,]  0.0448375907 -0.052671579  0.233692470  0.202388616\n[34,] -0.0988811205  0.114193534 -0.223377486 -0.352268664\n[35,] -0.0227001300 -0.060898077 -0.128031067  0.433279738\n[36,]  0.0198797585 -0.588908555 -0.205557660 -0.360835758\n[37,] -0.1810782317  0.169624547  0.047820491  0.128016136\n[38,] -0.5608910638  0.084877015  0.357326432 -0.103944632\n[39,]  0.1324105121 -0.084752687  0.010033062 -0.067306811\n[40,] -0.0371919548 -0.100855409 -0.123319327 -0.029662242\n             [,21]        [,22]         [,23]         [,24]\n [1,] -0.117087912 -0.041635527  0.0709019292  0.0414434673\n [2,] -0.104816705 -0.071697938 -0.1516926698  0.0752439758\n [3,] -0.218278498 -0.066920014 -0.0855536079 -0.2672630903\n [4,] -0.051573634  0.013024025  0.0100780342 -0.0643340039\n [5,] -0.003116800  0.431780926 -0.0002184472  0.0409079650\n [6,] -0.027579586 -0.580873577 -0.0088210753 -0.0609796246\n [7,] -0.304239249 -0.011680788  0.2713061148  0.0639331633\n [8,] -0.102301273  0.085446709  0.0097934282  0.3257353385\n [9,]  0.053148079  0.060657667  0.0355380478  0.0002935836\n[10,]  0.014027123 -0.053744281 -0.0203946660 -0.0083076139\n[11,] -0.365502706 -0.015439290  0.0696001284 -0.1367806341\n[12,] -0.100155475 -0.174031626 -0.0013288402 -0.0719639203\n[13,]  0.004755663  0.119849257 -0.0015151751  0.0167194001\n[14,]  0.000000000  0.000000000  0.0000000000  0.0000000000\n[15,] -0.110038254 -0.167073237  0.0744690593  0.0070437398\n[16,]  0.084162741 -0.322465687 -0.1223679961 -0.0498456836\n[17,]  0.082580486 -0.011500691 -0.1642471298 -0.0395687930\n[18,]  0.090319364  0.085083447 -0.0225560723  0.0447561092\n[19,] -0.040385814  0.080764742  0.0322537282 -0.5036333783\n[20,]  0.000000000  0.000000000  0.0000000000  0.0000000000\n[21,]  0.233089748  0.003873725 -0.2135449195  0.0884092975\n[22,]  0.002320884 -0.102019506 -0.0210738883  0.0952128999\n[23,]  0.107150723 -0.006325702 -0.0533168170  0.0264665767\n[24,] -0.086718806  0.235059637 -0.0540416957  0.0232666280\n[25,] -0.299857734 -0.031872975 -0.1715594215  0.2064849687\n[26,] -0.295414771  0.078041407 -0.0178865467  0.0357577703\n[27,]  0.000000000  0.000000000  0.0000000000  0.0000000000\n[28,]  0.000000000  0.000000000  0.0000000000  0.0000000000\n[29,]  0.290558707 -0.043236468  0.0977821533  0.0543170670\n[30,]  0.014027123 -0.053744281 -0.0203946660 -0.0083076139\n[31,]  0.032540907 -0.019093377  0.0630900778 -0.1978876823\n[32,]  0.013691184  0.091497689 -0.5673204386 -0.0946196647\n[33,] -0.006829714  0.076065002 -0.0657311466  0.2338329430\n[34,]  0.125812082 -0.026854392  0.2483013849 -0.2953461622\n[35,]  0.413684649 -0.112525858 -0.1730777520  0.2057704646\n[36,] -0.203341076 -0.134174760  0.5438382050  0.4365703710\n[37,]  0.141144379  0.085804195  0.1477316187 -0.1285090031\n[38,] -0.041274326 -0.292419256 -0.0197764761 -0.0235592941\n[39,]  0.138470411  0.205862555 -0.0511160776  0.0998751845\n[40,]  0.182172478 -0.034686420 -0.0360209006 -0.0492591835\n              [,25] [,26] [,27] [,28] [,29]         [,30]\n [1,] -1.528203e-16     0     0     0     0  5.598188e-17\n [2,]  6.797087e-02     0     0     0     0 -7.914394e-03\n [3,] -1.703293e-01     0     0     0     0 -1.115476e-01\n [4,]  8.500273e-17     0     0     0     0  3.331686e-17\n [5,]  8.992795e-17     0     0     0     0 -1.010755e-16\n [6,] -1.579719e-16     0     0     0     0 -1.491728e-16\n [7,]  4.975454e-01     0     0     0     0 -6.276306e-02\n [8,]  3.282352e-16     0     0     0     0 -2.236726e-16\n [9,]  1.054329e-16     0     0     0     0 -3.362569e-16\n[10,] -5.386548e-02     0     0     0     0 -7.484086e-01\n[11,]  2.654207e-16     0     0     0     0 -8.170181e-16\n[12,]  3.076778e-16     0     0     0     0 -9.689131e-17\n[13,]  1.721125e-17     0     0     0     0 -2.854105e-17\n[14,]  0.000000e+00     1     0     0     0  0.000000e+00\n[15,] -4.841029e-01     0     0     0     0  4.601690e-01\n[16,]  3.302017e-01     0     0     0     0 -1.272493e-01\n[17,]  3.027986e-16     0     0     0     0  1.587474e-16\n[18,]  6.767004e-17     0     0     0     0 -9.396244e-17\n[19,] -8.509389e-17     0     0     0     0 -4.203123e-17\n[20,]  0.000000e+00     0     1     0     0  0.000000e+00\n[21,] -1.703293e-01     0     0     0     0 -1.115476e-01\n[22,]  8.977284e-17     0     0     0     0  2.732035e-18\n[23,] -2.825585e-16     0     0     0     0  1.097840e-16\n[24,] -3.830374e-16     0     0     0     0  7.252601e-16\n[25,]  7.838245e-16     0     0     0     0  5.802081e-16\n[26,]  2.047169e-01     0     0     0     0  2.389240e-01\n[27,]  0.000000e+00     0     0     1     0  0.000000e+00\n[28,]  0.000000e+00     0     0     0     1  0.000000e+00\n[29,] -2.952545e-16     0     0     0     0 -9.535196e-16\n[30,] -4.070654e-01     0     0     0     0  1.234834e-01\n[31,] -4.309615e-16     0     0     0     0  2.769175e-16\n[32,]  1.019563e-01     0     0     0     0 -1.187159e-02\n[33,] -1.753415e-17     0     0     0     0 -9.241006e-17\n[34,]  2.726878e-01     0     0     0     0  2.310096e-01\n[35,]  1.931162e-16     0     0     0     0  3.922998e-16\n[36,] -6.797087e-02     0     0     0     0  7.914394e-03\n[37,] -6.797087e-02     0     0     0     0  7.914394e-03\n[38,]  2.076709e-16     0     0     0     0  1.087890e-16\n[39,] -2.047169e-01     0     0     0     0 -2.389240e-01\n[40,] -1.160633e-16     0     0     0     0  2.436694e-16\n              [,31]         [,32]         [,33]         [,34]\n [1,] -3.139976e-16 -4.166846e-16  3.720463e-16  0.1165420714\n [2,] -1.823746e-01 -2.290657e-01  1.854826e-01  0.3393604179\n [3,]  2.070984e-02  1.703515e-01 -1.573911e-01 -0.1401589344\n [4,] -1.119757e-16 -1.713237e-16  1.068731e-16  0.0202123626\n [5,] -1.825896e-16 -2.102054e-16  2.052095e-16  0.0002339602\n [6,] -2.176260e-17  1.862956e-16 -1.579297e-16 -0.0057779129\n [7,] -1.989480e-01 -3.300313e-01  2.394103e-01 -0.3327781584\n [8,] -8.830091e-17  5.061097e-17 -1.106381e-16  0.0488783863\n [9,] -2.514481e-16 -3.439303e-16  2.909493e-16  0.0238069812\n[10,] -6.385047e-01 -6.445100e-01  6.687868e-01  0.0089203309\n[11,] -8.907588e-16 -6.323736e-16  3.940622e-16  0.2105180578\n[12,]  8.019536e-18  1.820819e-17 -8.375762e-17  0.0301481031\n[13,] -7.833501e-17 -7.449417e-17  6.847827e-17 -0.0010836106\n[14,]  0.000000e+00  0.000000e+00  0.000000e+00  0.0000000000\n[15,]  3.357665e-01  1.490082e-01 -6.647356e-03 -0.1396280866\n[16,] -1.165690e-01 -5.159156e-02  4.786613e-03  0.0535219852\n[17,]  9.567801e-16  1.154401e-15 -1.001069e-15 -0.2431896230\n[18,]  8.644424e-18  9.902105e-17 -8.284937e-17  0.0158169498\n[19,] -1.016360e-16 -2.099437e-16  2.013968e-16  0.0033642733\n[20,]  0.000000e+00  0.000000e+00  0.000000e+00  0.0000000000\n[21,]  2.070984e-02  1.703515e-01 -1.573911e-01  0.4981901400\n[22,]  1.920633e-16  3.502284e-16 -3.214543e-16 -0.0035185060\n[23,]  3.163187e-16  3.997175e-16 -2.651141e-16 -0.0934669747\n[24,]  6.232144e-16  7.833968e-16 -6.895099e-16 -0.0351170261\n[25,]  7.984241e-16  7.642895e-16 -6.974811e-16 -0.0747119249\n[26,]  3.233295e-01  1.174284e-01 -5.618311e-02 -0.2481805423\n[27,]  0.000000e+00  0.000000e+00  0.000000e+00  0.0000000000\n[28,]  0.000000e+00  0.000000e+00  0.000000e+00  0.0000000000\n[29,] -1.052591e-15 -1.038994e-15  9.127499e-16  0.0213840619\n[30,]  1.482448e-01  2.779692e-01 -4.920818e-01  0.0089203309\n[31,] -5.461443e-17 -1.337507e-16  1.315250e-16  0.0839532972\n[32,] -2.735619e-01 -3.435985e-01  2.782240e-01  0.0579289994\n[33,]  9.585188e-17  2.405834e-16 -2.222337e-16 -0.1421150678\n[34,]  1.409549e-01 -1.116373e-01  1.292995e-01 -0.1021811361\n[35,]  1.201240e-15  1.165542e-15 -8.788262e-16 -0.2881065358\n[36,]  1.823746e-01  2.290657e-01 -1.854826e-01 -0.0085845371\n[37,]  1.823746e-01  2.290657e-01 -1.854826e-01 -0.3280063772\n[38,]  6.833734e-17 -1.910724e-17  2.291724e-17 -0.0097705439\n[39,] -3.233295e-01 -1.174284e-01  5.618311e-02  0.2061959329\n[40,]  2.692229e-16  1.607058e-16 -6.920952e-17 -0.1009431187\n              [,35]        [,36]        [,37]        [,38]\n [1,] -0.0031395968 -0.039340489 -0.061692404 -0.009882379\n [2,]  0.0126555942 -0.142659469 -0.018364832 -0.073735144\n [3,]  0.0472991837  0.272786840 -0.294291051 -0.272031349\n [4,]  0.0331504351 -0.020281369  0.178850347  0.093567010\n [5,]  0.0007616139 -0.011908463 -0.028102988 -0.056922879\n [6,] -0.0012555726  0.085750633  0.055465457  0.127375137\n [7,] -0.0364338368  0.097361582 -0.235325305 -0.139581963\n [8,] -0.0565917722 -0.127472629  0.148687311  0.132867247\n [9,]  0.0885599847 -0.274884135 -0.014080655 -0.140899559\n[10,]  0.0078929072 -0.009038436  0.012017163  0.020500968\n[11,] -0.0140653034 -0.362841105 -0.070229975 -0.172097089\n[12,] -0.4286441516 -0.122360348 -0.097722164 -0.193481249\n[13,] -0.0143632037  0.003765589 -0.018266405 -0.027485131\n[14,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[15,]  0.0054618033  0.021565483 -0.081611165 -0.008288079\n[16,]  0.0473574435 -0.054230617  0.072102975  0.123005806\n[17,]  0.0662463569  0.121837326  0.323475780  0.016374497\n[18,]  0.0106834171 -0.020421258  0.345407390  0.238620665\n[19,]  0.0192010493 -0.027369401 -0.118374958 -0.071475416\n[20,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[21,] -0.0331146676 -0.290470198 -0.216315912 -0.143612821\n[22,] -0.0145976777  0.038852780  0.022358192  0.040900217\n[23,]  0.0143605678  0.214276752 -0.002397322  0.037463018\n[24,] -0.5377165290  0.401100620  0.037336627  0.298056624\n[25,] -0.0493755150  0.011123602 -0.280997346 -0.122143705\n[26,]  0.2355497144  0.293698052  0.064289778  0.144316317\n[27,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[28,]  0.0000000000  0.000000000  0.000000000  0.000000000\n[29,] -0.0855144379 -0.260087486  0.152563500  0.399945436\n[30,]  0.0078929072 -0.009038436  0.012017163  0.020500968\n[31,]  0.0287006289  0.194438484 -0.344991497 -0.272269476\n[32,]  0.0160435120 -0.186514438 -0.029418161 -0.061014374\n[33,]  0.0287350854  0.190507183  0.168443825  0.240608609\n[34,]  0.0705605512 -0.069297879 -0.117032604 -0.198900028\n[35,] -0.0222929751  0.163727119 -0.376598587 -0.302657971\n[36,]  0.0027047435  0.020053891  0.165079055  0.117120779\n[37,] -0.0387883935  0.053387587  0.022008825  0.023509419\n[38,]  0.4449777552  0.053888889 -0.021336003 -0.129348723\n[39,]  0.4794630471 -0.093839604  0.207184743  0.287942116\n[40,] -0.0060400943  0.161578299 -0.028894781  0.005404070\n             [,39]        [,40]\n [1,] -0.054938552 -0.079832424\n [2,]  0.228246023 -0.250530656\n [3,] -0.234713514  0.030477469\n [4,] -0.006999792 -0.003518314\n [5,] -0.037623553 -0.001255277\n [6,]  0.092195131  0.004614036\n [7,]  0.011435155  0.008621538\n [8,]  0.148671552  0.069125443\n [9,] -0.098581132 -0.030281912\n[10,]  0.017962346 -0.003566323\n[11,] -0.110308448  0.429781766\n[12,] -0.104989449  0.077675478\n[13,] -0.021990552  0.002910720\n[14,]  0.000000000  0.000000000\n[15,]  0.059604616 -0.006388200\n[16,]  0.107774077 -0.021397937\n[17,]  0.082796446  0.340268813\n[18,]  0.045347044 -0.095736743\n[19,]  0.095826419 -0.054327536\n[20,]  0.000000000  0.000000000\n[21,]  0.013880171  0.204178371\n[22,]  0.085805503 -0.078844133\n[23,] -0.523930070  0.189677316\n[24,]  0.188229497  0.047085333\n[25,] -0.093193405 -0.037418360\n[26,]  0.085757593 -0.095170888\n[27,]  0.000000000  0.000000000\n[28,]  0.000000000  0.000000000\n[29,]  0.206932681 -0.090642740\n[30,]  0.017962346 -0.003566323\n[31,] -0.193112965 -0.152566299\n[32,] -0.140175482  0.193205535\n[33,]  0.189984590 -0.270941905\n[34,] -0.193111109  0.319537193\n[35,] -0.152353877  0.301164562\n[36,] -0.190709122  0.222598362\n[37,]  0.413179741  0.095722010\n[38,] -0.065930201 -0.004073826\n[39,]  0.221280185  0.298585551\n[40,] -0.002993332 -0.159607323\n\nPlot your network so that the sizes of the nodes are proportional to their centrality as given by Gould’s index\n\n\n\nSort and rank your vertices according to eigenvalue centrality and according to degree centrality.\nDegree Centrality\n\n\ndf=data.frame(u)\nrownames(df) = DepartmentName\nii=order(u,decreasing=TRUE)\ndf2=data.frame(df[ii,])\nrownames(df2) = DepartmentName[ii]\ndf2\n\n\n                                           df.ii...\nEnvironmental Studies                   0.106666667\nAmerican Studies                        0.080000000\nHistory                                 0.066666667\nWomen's, Gender, and Sexuality Studies  0.062222222\nPolitical Science                       0.057777778\nLatin American Studies                  0.053333333\nAsian Studies                           0.044444444\nBiology                                 0.044444444\nClassical Mediterranean and Middle East 0.026666667\nComputer Science                        0.026666667\nEnglish                                 0.026666667\nGeology                                 0.026666667\nInternational Studies                   0.026666667\nPsychology                              0.026666667\nMedia and Culutral Studies              0.022222222\nPhilosophy                              0.022222222\nReligious Studies                       0.022222222\nSociology                               0.022222222\nSpanish                                 0.022222222\nStatistics                              0.022222222\nTheater and Dance                       0.022222222\nArt and Art History                     0.017777778\nGeography                               0.017777778\nGerman Studies                          0.017777778\nAnthropology                            0.013333333\nChemistry                               0.013333333\nJapanese                                0.013333333\nMathematics                             0.013333333\nRussian Studies                         0.013333333\nChinese                                 0.008888889\nEducational Studies                     0.008888889\nLinguistics                             0.008888889\nPortugese                               0.008888889\nEconomics                               0.004444444\nMusic                                   0.004444444\nPhysics and Astronomy                   0.004444444\nFrench and Francophone Studies          0.000000000\nInterdisciplinary Studies               0.000000000\nNeuroscience                            0.000000000\nPhysical Education                      0.000000000\n\nEigenvalue centrality\n\n\ndf = data.frame(w) \nrownames(df)=DepartmentName\nii=order(w,decreasing=TRUE)\ndf2 = data.frame(df[ii,])\nrownames(df2) = DepartmentName[ii]\ndf2\n\n\n                                           df.ii...\nEnvironmental Studies                   0.129465704\nPolitical Science                       0.093655804\nAmerican Studies                        0.082649618\nBiology                                 0.076833790\nGeology                                 0.070131899\nHistory                                 0.066139198\nLatin American Studies                  0.059597324\nGeography                               0.037426081\nSociology                               0.032109360\nAsian Studies                           0.026141263\nMedia and Culutral Studies              0.024763528\nGerman Studies                          0.024286781\nWomen's, Gender, and Sexuality Studies  0.024046298\nEnglish                                 0.024040463\nPhilosophy                              0.021119962\nChemistry                               0.020810529\nSpanish                                 0.019470184\nInternational Studies                   0.018560317\nTheater and Dance                       0.016144825\nReligious Studies                       0.013705363\nRussian Studies                         0.013618285\nComputer Science                        0.011799827\nEconomics                               0.011688650\nPhysics and Astronomy                   0.011688650\nPortugese                               0.010761340\nClassical Mediterranean and Middle East 0.010530491\nPsychology                              0.010197584\nEducational Studies                     0.008382594\nJapanese                                0.006891253\nArt and Art History                     0.006432586\nChinese                                 0.004720263\nStatistics                              0.003502562\nAnthropology                            0.002808020\nMusic                                   0.002170463\nLinguistics                             0.002011360\nMathematics                             0.001697782\nFrench and Francophone Studies          0.000000000\nInterdisciplinary Studies               0.000000000\nNeuroscience                            0.000000000\nPhysical Education                      0.000000000\n\nwhich nodes are most and least central in each of the measures of centrality\nIn the eigenvalue centrality graph, environmental studies is the most central while French and Francophone Studies, Interdisciplinary Studies, Neuroscience, and Physical Education are the least central.\nIn the degree centrality graph, environmental studies is the most central while French and Francophone Studies, Interdisciplinary Studies, Neuroscience, and Physical Education are the least central.\nany other interesting/surprising observations from your centrality measures\nThere were four departments that were not connected to any other department.\nThere are two distinct groups with high centrality: environmental studies, geology, and biology and American studies, political science, and history.\n\n\n\n",
    "preview": "posts/LinearFinalProject/MacalesterDepartment_post_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-12-16T23:47:37-06:00",
    "input_file": {}
  }
]
